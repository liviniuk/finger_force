{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import glob\n",
    "from skimage import io, transform\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-image CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 3\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GripForceDataset(Dataset):\n",
    "    \"\"\"Finger Grip Force dataset.\"\"\"\n",
    "    def __init__(self, root_dir, n_subj, n_exp_per_subj, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            n_subj (int): Number of test subjects.\n",
    "            n_exp_per_subj (int): Number of experiments per test subject.\n",
    "            k: TODO (number of frames to use as one item)\n",
    "        \"\"\"\n",
    "        self.N = 0\n",
    "        self.images = np.zeros((0, 3), dtype=np.int)\n",
    "        self.force = np.zeros((0, 2), dtype=np.float)\n",
    "        for subject_number in range(1, n_subj + 1):\n",
    "            for experiment_number in range(1, n_exp_per_subj + 1):\n",
    "                img_path = '{}/{:02d}/{:02d}/frames_aligned/'.format(root_dir, subject_number, experiment_number)\n",
    "                force_path = '{}/{:02d}/{:02d}/labels.csv'.format(root_dir, subject_number, experiment_number)\n",
    "        \n",
    "                n = len(glob.glob('{}*.png'.format(img_path)))\n",
    "                if (n >= 5):\n",
    "                    experiment = np.zeros((n - 4, 3), dtype=np.int)\n",
    "                    experiment[:,0] = subject_number\n",
    "                    experiment[:,1] = experiment_number\n",
    "                    # central fram of the item (item = frame + 2 prev + 2 next frames)\n",
    "                    experiment[:,2] = np.arange(2, n - 2)\n",
    "                    self.images = np.concatenate((self.images, experiment), axis=0)\n",
    "                    \n",
    "                    force = np.loadtxt(force_path, delimiter=',')[2:-2,:]\n",
    "                    self.force = np.concatenate((self.force, force), axis=0)\n",
    "                    \n",
    "                    self.N += n - 4\n",
    "                    \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print('__getitem__ {}'.format(idx))\n",
    "        \n",
    "#         img_paths = ['{}/{:02d}/{:02d}/frames_aligned/{:04d}.png'.format(self.root_dir, \n",
    "#                                                                     self.images[idx, 0], \n",
    "#                                                                     self.images[idx, 1], \n",
    "#                                                                     self.images[idx, 2] + x) for x in range(-2, 3)]\n",
    "#         frames = np.concatenate([io.imread(img_path) for img_path in img_paths], axis=2)\n",
    "        img_path = '{}/{:02d}/{:02d}/frames_aligned/{:04d}.png'.format(self.root_dir, \n",
    "                                                                    self.images[idx, 0], \n",
    "                                                                    self.images[idx, 1], \n",
    "                                                                    self.images[idx, 2])\n",
    "        frames = np.array(io.imread(img_path))\n",
    "\n",
    "        force = self.force[idx] / 1000.0 - 0.5\n",
    "        \n",
    "        sample = {'frames': frames, 'force': force}\n",
    "        if self.transform:\n",
    "#             print('__transform {}'.format(idx))\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "#         print('__returning {}'.format(idx))\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Dataset class\n",
    "root_dir = '/media/viktor/Samsung_T5/Research/dataset'\n",
    "n_subj = 1\n",
    "n_exp_per_subj = 10\n",
    "\n",
    "ds = GripForceDataset(root_dir, n_subj, n_exp_per_subj)\n",
    "\n",
    "print('len={}'.format(ds.__len__()))\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "sample = ds[105]\n",
    "for i in range(1):\n",
    "#     print(i, sample['image'].shape, sample['landmarks'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 6, i + 1)\n",
    "    ax.set_title('frame#{}'.format(i-2))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample['frames'][:,:,i*3:(i+1)*3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        frames, force = sample['frames'], sample['force']\n",
    "\n",
    "        h, w = frames.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        frames = transform.resize(frames, (new_h, new_w))\n",
    "\n",
    "        return {'frames': frames, 'force': force}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        frames, force = sample['frames'], sample['force']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        frames = frames.transpose((2, 0, 1))\n",
    "        return {'frames': torch.from_numpy(frames),\n",
    "                'force': torch.from_numpy(force)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Dataset with transform\n",
    "transformed_dataset = GripForceDataset(root_dir, \n",
    "                                       n_subj, \n",
    "                                       n_exp_per_subj,\n",
    "                                       transform=T.Compose([\n",
    "#                                            Rescale(270),\n",
    "                                           Rescale(135),\n",
    "                                           ToTensor()\n",
    "                                       ]))\n",
    "\n",
    "for i in range(len(transformed_dataset)):\n",
    "    sample = transformed_dataset[i]\n",
    "\n",
    "    print(i, sample['frames'].size(), sample['force'].size())\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 32\n",
    "val_split = .05\n",
    "test_split = .1\n",
    "shuffle_dataset = True # Only for train/val - test dataset is not shuffled\n",
    "random_seed = 42\n",
    "\n",
    "# Data indices\n",
    "dataset_size = len(transformed_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "val_size = int(np.floor(val_split * dataset_size))\n",
    "test_size = int(np.floor(test_split * dataset_size))\n",
    "train_size = dataset_size - val_size - test_size\n",
    "\n",
    "test_indices = indices[train_size+val_size:]\n",
    "indices = indices[:train_size+val_size]\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# # JUST FOR DEBUG PURPOSE\n",
    "# batch_size = 3\n",
    "# train_indices = train_indices[:6]\n",
    "# val_indices = val_indices[:6]\n",
    "# test_indices = test_indices[:6]\n",
    "\n",
    "# Data Samplers\n",
    "train_sampler = sampler.SubsetRandomSampler(train_indices)\n",
    "val_sampler = sampler.SubsetRandomSampler(val_indices)\n",
    "test_sampler = sampler.SequentialSampler(test_indices)\n",
    "\n",
    "# Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(transformed_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           num_workers=num_workers,\n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(transformed_dataset,\n",
    "                                         batch_size=1,\n",
    "                                         num_workers=num_workers,\n",
    "                                         sampler=val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(transformed_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=num_workers,\n",
    "                                          sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, maps_1, maps_2, maps_3, neurons_1, neurons_2, num_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, maps_1, (7, 7), padding=(3,3))\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(maps_1, maps_2, (9, 9), padding=(4,4))\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(maps_2, maps_3, (5, 5), padding=(2,2))\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8736, num_out)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(8736, neurons_1)\n",
    "#         nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(neurons_1, neurons_2)\n",
    "#         nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        \n",
    "#         self.fc3 = nn.Linear(neurons_2, num_out)\n",
    "#         nn.init.kaiming_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 3)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 3)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "#         print(num_features)\n",
    "        return num_features\n",
    "\n",
    "\n",
    "def test_ConvNet():\n",
    "    x = torch.zeros((2, 3, 135, 240), dtype=dtype)  # (minibatch size, image size (ch, h, w))\n",
    "    model = ConvNet(in_channels=3, maps_1=32, maps_2=64, maps_3=96, neurons_1=100, neurons_2 = 100, num_out=2)\n",
    "    scores = model(x)\n",
    "    print(scores.size())\n",
    "test_ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet(in_channels=3, maps_1=32, maps_2=64, maps_3=96, neurons_1=100, neurons_2 = 100, num_out=2)\n",
    "# net = net.to(device=device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model, train=False):\n",
    "    if train == True:\n",
    "        print('Checking accuracy on validation set')\n",
    "        n = len(loader.sampler.indices)\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "        n = len(loader.sampler.data_source)\n",
    "        \n",
    "    y_true = np.zeros((n, 2), dtype=np.float)\n",
    "    y_pred = np.zeros((n, 2), dtype=np.float)\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        k = 0\n",
    "        it = iter(loader)\n",
    "        sample = next(it, None)\n",
    "        while sample != None:\n",
    "            x, y = sample['frames'], sample['force']\n",
    "            sample = next(it, None)\n",
    "            \n",
    "            # move to device, e.g. GPU\n",
    "            x = x.to(device=device, dtype=dtype)  \n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            \n",
    "            # predict\n",
    "            ﾑダpred_gpu = model(x)\n",
    "            \n",
    "            # store delta\n",
    "            l = y.shape[0]\n",
    "            y_pred_cpu = ﾑダpred_gpu.data.cpu()\n",
    "            y_pred[k:k+l, :] = y_pred_cpu.numpy()\n",
    "            y_true[k:k+l, :] = y.numpy()\n",
    "            k += l\n",
    "    \n",
    "    y_true = (y_true + 0.5) * 1000.0\n",
    "    y_pred = (y_pred + 0.5) * 1000.0\n",
    "    \n",
    "    r2_thumb = r2_score(y_true[:,0].flatten(), y_pred[:,0].flatten())\n",
    "    r2_point = r2_score(y_true[:,1].flatten(), y_pred[:,1].flatten())\n",
    "#     r2 = r2_score(y_true, y_pred)\n",
    "    MSE = mean_squared_error(y_true, y_pred)\n",
    "    print('RMSE: {:.2f}  R2 thumb score: {:.2f}  R2 point score: {:.2f}'.format(MSE ** .5, r2_thumb, r2_point))\n",
    "#     print('R2 score: {:.2f}'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "check_accuracy(val_loader, net, True)\n",
    "t1 = time.time()\n",
    "print('It took {:.2f} sec'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on Force dataset using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        it = iter(train_loader)\n",
    "        sample = next(it, None)\n",
    "        while sample != None:\n",
    "            x, y = sample['frames'], sample['force']\n",
    "            sample = next(it, None)\n",
    "            \n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            \n",
    "            # forward\n",
    "            y_pred = model(x)\n",
    "            mse = nn.MSELoss()\n",
    "            loss = mse(y_pred, y)\n",
    "            \n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "        print('Epoch {}, loss = {:.4f}'.format(epoch, loss.item()))\n",
    "        check_accuracy(val_loader, model, train=True)\n",
    "        print(\"Epoch time: {:.1f} sec\\n\".format(time.time() - t0))\n",
    "        \n",
    "        global loss_stat\n",
    "        loss_stat.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "\n",
    "loss_stat = []\n",
    "\n",
    "model = ConvNet(in_channels=3, maps_1=32, maps_2=64, maps_3=96, neurons_1=100, neurons_2 = 100, num_out=2)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.95, momentum=0.9, eps=0.0001)\n",
    "\n",
    "train(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(loss_stat) + 1), loss_stat)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(test_loader, model, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(val_loader, model, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(train_loader, model, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/viktor/Samsung_T5/Research/models/01/first_model.pth.tar\"\n",
    "path_dict = \"/media/viktor/Samsung_T5/Research/models/01/first_dict.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, path)\n",
    "torch.save(model.state_dict(), path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/viktor/Samsung_T5/Research/models/01/first_model.pth.tar\"\n",
    "model1 = ConvNet(in_channels=3, maps_1=32, maps_2=64, maps_3=96, neurons_1=100, neurons_2 = 100, num_out=2)\n",
    "model1 = torch.load(path)\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data4visualization(loader, model, fps=30.0):\n",
    "    n = len(loader.sampler.data_source)\n",
    "    \n",
    "    t = np.arange(n, dtype=np.float) / fps\n",
    "    \n",
    "    y_true = np.zeros((n, 2), dtype=np.float)\n",
    "    y_pred = np.zeros((n, 2), dtype=np.float)\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        k = 0\n",
    "        it = iter(loader)\n",
    "        sample = next(it, None)\n",
    "        while sample != None:\n",
    "            x, y = sample['frames'], sample['force']\n",
    "            sample = next(it, None)\n",
    "            \n",
    "            # move to device, e.g. GPU\n",
    "            x = x.to(device=device, dtype=dtype)  \n",
    "#             y = y.to(device=device, dtype=dtype)\n",
    "            \n",
    "            # predict\n",
    "            ﾑダpred_gpu = model(x)\n",
    "            \n",
    "            # store delta\n",
    "            l = y.shape[0]\n",
    "            y_pred_cpu = ﾑダpred_gpu.data.cpu()\n",
    "            y_pred[k:k+l, :] = y_pred_cpu.numpy()\n",
    "            y_true[k:k+l, :] = y.numpy()\n",
    "            k += l\n",
    "    \n",
    "    y_true = (y_true + 0.5) * 1000.0\n",
    "    y_pred = (y_pred + 0.5) * 1000.0\n",
    "    \n",
    "    return t, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y_true, y_pred = data4visualization(test_loader, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(t, y_true, y_pred, thumb=True, both=False, linewidth=2):\n",
    "    plt.figure(figsize=(20,15))\n",
    "    if thumb or both:\n",
    "        plt.plot(t, y_true[:,0], 'r-', linewidth=linewidth, label='Thumb, ground truth')\n",
    "        plt.plot(t, y_pred[:,0], 'b-', linewidth=linewidth, label='Thumb, prediction')\n",
    "    if not thumb or both:\n",
    "        plt.plot(t, y_true[:,1], 'g-', linewidth=linewidth, label='Index, ground truth')\n",
    "        plt.plot(t, y_pred[:,1], 'k-', linewidth=linewidth, label='Index, prediction')\n",
    "    plt.ylabel('Force sensitive resistor data', fontsize=24)\n",
    "    plt.xlabel('Time [sec]', fontsize=24)\n",
    "    plt.grid()\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(t, y_true, y_pred, thumb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(t, y_true, y_pred, thumb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(t, y_true, y_pred, both=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Thumb')\n",
    "print('RMSE: {:.2f}'.format(mean_squared_error(y_true[:,0], y_pred[:,0]) ** .5))\n",
    "print('R2 score: {:.2f}'.format(r2_score(y_true[:,0], y_pred[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Index')\n",
    "print('RMSE: {:.2f}'.format(mean_squared_error(y_true[:,1], y_pred[:,1]) ** .5))\n",
    "print('R2 score: {:.2f}'.format(r2_score(y_true[:,1], y_pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
